import pandas as pd
import numpy as np
import sys
from datetime import datetime
from typing import Dict, Optional, List
from functools import lru_cache

# --- Visualization ---
try:
    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D
    import seaborn as sns
    VISUALIZATION_AVAILABLE = True
except ImportError:
    VISUALIZATION_AVAILABLE = False
    print("⚠️ WARNING: Visualization libraries (matplotlib, seaborn) not found. Plots will not be generated.")

# --- Step 1: Check for Bloomberg BQL Availability ---
print("--- Initializing Prism Verification v3.0 (Stable) ---")
try:
    import bql
    BQL_SERVICE = bql.Service()
    BQL_AVAILABLE = True
    print("✅ Bloomberg BQL module imported successfully. Live data mode is active.")
except ImportError:
    BQL_AVAILABLE = False
    BQL_SERVICE = None
    print("⚠️ WARNING: Bloomberg BQL module not found. Will fall back to synthetic data.")


# ==============================================================================
# --- CONFIGURATION: The Single Source of Truth for Your Data Universe ---
# ==============================================================================
CONFIG = {
    'universe': {
        'currencies': ['USD', 'EUR', 'JPY', 'GBP'],
        'base_currency': 'USD',
        'tenors': {'1Y': 365, '2Y': 730, '3Y': 1095},
    },
    'date_range': {
        'start': '2007-01-01',
        'end': '2012-12-31',
    },
    'tickers': {
        'spot': { 'EUR': 'EURUSD Curncy', 'JPY': 'JPYUSD Curncy', 'GBP': 'GBPUSD Curncy' },
        'swaps': {
            'EUR': {'1Y': 'EUSA1 Curncy', '2Y': 'EUSA2 Curncy', '3Y': 'EUSA3 Curncy'},
            'JPY': {'1Y': 'JYSA1 Curncy', '2Y': 'JYSA2 Curncy', '3Y': 'JYSA3 Curncy'},
            'GBP': {'1Y': 'BPSA1 Curncy', '2Y': 'BPSA2 Curncy', '3Y': 'BPSA3 Curncy'},
        },
        'ois_curves': {
            'USD': 'YCSW0023 Index', 'EUR': 'YCSW0004 Index',
            'JPY': 'YCSW0007 Index', 'GBP': 'YCSW0015 Index',
        },
        'stress_indicator': 'VIX Index'
    },
    'constants': { 'swap_points_scale': 10000.0, 'days_in_year': 365.0 },
    'plot_parameters': {
        'geometric_warp_date': '2008-09-30', 'surface_plot_currency': 'EUR',
    }
}

# =================================================================================
# --- DEFINITIVE DATA LOADING & PROCESSING (FETCH-THEN-INTERPOLATE STRATEGY) ---
# =================================================================================

# LRU Cache automatically stores results of this function to avoid re-fetching data.
@lru_cache(maxsize=None)
def get_raw_curve_points(curve_ticker: str, query_date: str, bql_service: 'bql.Service') -> Optional[pd.DataFrame]:
    """
    For a single curve on a single date, fetches all constituent member points.
    This is the robust way to get curve data.
    """
    try:
        # Define the universe as the members of the curve on a specific date
        curve_members = bql_service.univ.curvemembers(curve_ticker, dates=query_date)
        
        # Define the data items to retrieve for each member of the curve
        data_items = {
            # Use 'duration_in_days' for a clean numeric x-axis for interpolation
            'days': bql_service.data.duration(to_bd='Y') * 365.25,
            'rate': bql_service.data.curve_rate(sides='mid')
        }
        
        request = bql.Request(curve_members, data_items, with_params={'dates': query_date})
        response = bql_service.execute(request)
        
        # Combine the results into a single, clean DataFrame
        df = pd.concat([item.df() for item in response], axis=1).dropna().sort_values('days')
        return df
    except Exception as e:
        print(f"    ⚠️ Could not fetch curve points for {curve_ticker} on {query_date}. Error: {e}")
        return None

def interpolate_rates_for_dates(dates: pd.DatetimeIndex, curve_ticker: str, target_tenors_days: List[int], bql_service: 'bql.Service') -> pd.DataFrame:
    """
    For a given curve and a range of dates, fetches raw curve points and interpolates
    to find rates at the specified target tenors.
    """
    all_interpolated_rates = []
    for date in dates:
        date_str = date.strftime('%Y-%m-%d')
        raw_points = get_raw_curve_points(curve_ticker, date_str, bql_service)
        
        if raw_points is not None and not raw_points.empty and len(raw_points) > 1:
            # Use numpy's linear interpolation
            x = raw_points['days'].values
            y = raw_points['rate'].values
            interpolated = np.interp(target_tenors_days, x, y)
            all_interpolated_rates.append(interpolated)
        else:
            # Append NaNs if data for that day is missing, to be filled later
            all_interpolated_rates.append([np.nan] * len(target_tenors_days))
            
    # Create a DataFrame from the results
    df_interpolated = pd.DataFrame(all_interpolated_rates, index=dates, columns=config['universe']['tenors'].keys())
    return df_interpolated

def load_and_prepare_market_data(config: Dict, bql_service: 'bql.Service') -> Optional[Dict[str, pd.DataFrame]]:
    """[LIVE DATA] Handles data fetching and structuring using robust methods."""
    print("\n--- [PHASE 1 - LIVE] FETCHING & STRUCTURING MARKET DATA ---")
    try:
        # 1. Fetch simple price-based data (Spot, Swaps, VIX)
        print("  - Fetching Spot, Swap, and VIX prices...")
        all_px_tickers = (
            list(config['tickers']['spot'].values()) +
            [t for ccy_swaps in config['tickers']['swaps'].values() for t in ccy_swaps.values()] +
            [config['tickers']['stress_indicator']]
        )
        date_range = bql_service.func.range(config['date_range']['start'], config['date_range']['end'])
        px_request = bql.Request(all_px_tickers, {'PX_LAST': bql_service.data.px_last(fill='prev')}, with_params={'dates': date_range})
        px_response = bql_service.execute(px_request)
        df_px_pivot = px_response[0].df().reset_index().pivot(index='DATE', columns='ID', values='PX_LAST')
        df_vix = df_px_pivot[[config['tickers']['stress_indicator']]].rename(columns={config['tickers']['stress_indicator']: 'VIX'})
        
        # 2. Fetch and interpolate OIS curves
        print("  - Fetching and interpolating OIS curves (this may take a moment)...")
        all_ois_dfs = []
        target_tenors_days = list(config['universe']['tenors'].values())
        
        for ccy, curve_ticker in config['tickers']['ois_curves'].items():
            print(f"    - Processing {ccy} curve ({curve_ticker})...")
            df_interpolated = interpolate_rates_for_dates(df_px_pivot.index, curve_ticker, target_tenors_days, bql_service)
            df_interpolated.columns = pd.MultiIndex.from_product([[curve_ticker], df_interpolated.columns], names=['ID', 'tenor'])
            all_ois_dfs.append(df_interpolated)
            
        df_ois_pivot = pd.concat(all_ois_dfs, axis=1) / 100.0

        # 3. Assemble the final multi-index DataFrame
        print("  - Assembling final market data DataFrame...")
        final_data_frames = []
        ccy_pairs = [f"{ccy}{config['universe']['base_currency']}" for ccy in config['universe']['currencies'] if ccy != config['universe']['base_currency']]
        for pair in ccy_pairs:
            ccy1, ccy2 = pair[:3], pair[3:]
            df_pair = pd.DataFrame(index=df_px_pivot.index)
            df_pair['Pair'] = pair
            df_pair['Spot'] = df_px_pivot[config['tickers']['spot'][ccy1]]
            for tenor in config['universe']['tenors']:
                df_pair[f'SwapPoints_{tenor}'] = df_px_pivot[config['tickers']['swaps'][ccy1][tenor]]
                df_pair[f'OIS_{ccy1}_{tenor}'] = df_ois_pivot[(config['tickers']['ois_curves'][ccy1], tenor)]
                df_pair[f'OIS_{ccy2}_{tenor}'] = df_ois_pivot[(config['tickers']['ois_curves'][ccy2], tenor)]
            final_data_frames.append(df_pair)

        df_market_data = pd.concat(final_data_frames).reset_index().set_index(['DATE', 'Pair'])
        # Use forward-fill to handle any gaps from interpolation failures on non-business days
        df_market_data = df_market_data.ffill().dropna()

        print("✅ Live data fetching and structuring successful.")
        return {'market_data': df_market_data, 'vix': df_vix}
    except Exception as e:
        print(f"❌ ERROR: A critical error occurred during live data fetching. Details: {e}")
        return None

# The PrismVerifier and other functions remain unchanged as they consume the final, correctly formatted DataFrame.
# (Code for PrismVerifier, synthetic data generation, and main execution block would follow here, unchanged.)
# NOTE: To keep the response focused on the fix, the rest of the unchanged code is omitted.
