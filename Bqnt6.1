# ========================================================================================
# --- KSI PHASE TRANSITION ANALYZER (v13.0 - Visual Intelligence) ---
#
# PURPOSE:
# This version enhances the KSI model with a powerful visual intelligence layer.
# It uses a specialized set of interest rate volatility assets to detect stress
# between short-term "Policy Jitters" and long-term "Structural Fear".
#
# METHODOLOGY:
# 1. CONFIGURE: The system is configured with the 4-asset interest rate vol quartet.
# 2. CALCULATE: It computes the 20-dimensional kinetic state vector and the KSI.
# 3. ANALYZE (TEXT): It provides a console-based report for top stress peaks.
# 4. VISUALIZE (NEW): It generates a suite of "Visual Intelligence" plots:
#    - Regime Fingerprints: A 4-panel dashboard showing system evolution.
#    - Event Gallery: A side-by-side comparison of top stress events via network diagrams.
#    - Detailed Event Analysis: A deep-dive visual report for each major peak.
# ========================================================================================

print("\n--- RUNNING KSI PHASE TRANSITION ANALYZER (v13.0 - Visual Intelligence) ---\n")

import pandas as pd
import numpy as np
import itertools
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import traceback
from datetime import datetime
import math
import networkx as nx

# --- Suppress Warnings for Cleaner Output ---
warnings.filterwarnings('ignore')

# --- Check BQL availability ---
BQL_AVAILABLE = False
try:
    import bql
    BQL_AVAILABLE = True
    print("✓ BQL module imported successfully.")
except ImportError:
    print("⚠️  BQL module not found. Set 'test_mode' in CONFIG to True to run with synthetic data.")

# ==============================================================================
# --- CONFIGURATION: The "Interest Rate Volatility Tension Matrix" ---
# ==============================================================================
CONFIG = {
    'assets': {
        'USDV3M2Y': {'primary': 'USDV3M2Y Index', 'asset_type': 'price', 'group': 'PolicyJitters'},
        'USDV1Y5Y': {'primary': 'USDV1Y5Y Index', 'asset_type': 'price', 'group': 'PolicyJitters'},
        'USDV5Y10Y': {'primary': 'USDV5Y10Y Index', 'asset_type': 'price', 'group': 'StructuralFear'},
        'USDV10Y30Y': {'primary': 'USDV10Y30Y Index', 'asset_type': 'price', 'group': 'StructuralFear'},
    },
    'date_range': {
        'start': '2010-01-01',
        'end': pd.to_datetime('today').strftime('%Y-%m-%d'),
    },
    'model_params': {
        'correlation_window': 60,
        'correlation_method': 'spearman',
        'mahalanobis_lookback': 252,
        'max_correlation_clip': 0.999,
    },
    'analysis_params': {
        'num_peaks_to_analyze': 5,
        'peak_separation_days': 180,
        'contamination_threshold_pct': 50.0,
    },
    'visual_intelligence': {
        'create_regime_fingerprints': True,
        'create_event_gallery': True,
    },
    'execution': {
        'test_mode': True, # <<<< SET TO True IF TESTING WITHOUT BLOOMBERG
        'verbose': True,
    }
}

# --- GIS Function (Required for robust covariance) ---
def GIS(Y, k=None):
    if Y.shape[0] < Y.shape[1]:
        raise ValueError(f"GIS failed: Not enough observations ({Y.shape[0]}) for number of features ({Y.shape[1]})")
    N, p = Y.shape
    if isinstance(Y, pd.DataFrame): Y = Y.to_numpy()
    if k is None or math.isnan(k): Y, k = Y - Y.mean(axis=0), 1
    n, c = N - k, p / n
    if p > n: raise ValueError(f"GIS failed: p={p} > n={n}")
    sample = (Y.T @ Y) / n
    sample = (sample + sample.T) / 2
    lambda1, u = np.linalg.eigh(sample)
    lambda1 = np.maximum(lambda1, 1e-12) # Epsilon for stability
    h = min(c**2, 1/c**2)**0.35 / p**0.35
    invlambda = 1 / lambda1
    Lj = np.tile(invlambda, (p, 1)).T
    Lj_i = Lj - Lj.T
    theta = np.mean(Lj * Lj_i / (Lj_i**2 + (Lj**2) * h**2), axis=0)
    Htheta = np.mean(Lj * Lj * h / (Lj_i**2 + (Lj**2) * h**2), axis=0)
    Atheta2 = theta**2 + Htheta**2
    deltahat_1 = (1 - c) * invlambda + 2 * c * invlambda * theta
    delta = 1 / ((1 - c)**2 * invlambda + 2 * c * (1 - c) * invlambda * theta + c**2 * invlambda * Atheta2)
    deltaLIS_1 = np.maximum(deltahat_1, np.min(invlambda[invlambda > 0]))
    sigmahat = u @ np.diag(np.sqrt(delta / deltaLIS_1)) @ u.T.conj() @ u @ np.diag(np.sqrt(delta / deltaLIS_1)) @ u.T.conj()
    return (sigmahat + sigmahat.T) / 2


class KineticPhaseTransitionAnalyzer:
    """
    Detects market phase transitions by analyzing the kinetic energy of a
    specialized asset quartet and generates rich visual intelligence reports.
    """
    def __init__(self, config):
        self.config = config
        self.bql_svc = None
        # Map long asset names to short, plot-friendly names
        self.short_names = {
            'USDV3M2Y': '3M2Y', 'USDV1Y5Y': '1Y5Y',
            'USDV5Y10Y': '5Y10Y', 'USDV10Y30Y': '10Y30Y',
        }
        self._log(f"Initializing Analyzer v13.0 with assets: {list(config['assets'].keys())}")

    def run(self):
        """Executes the full analysis and visualization pipeline."""
        if not self._validate_environment(): return
        
        price_df = self._acquire_data()
        if price_df is None: return

        returns_df = self._process_data(price_df)
        if returns_df is None: return

        self.state_vector_df = self._build_state_vector(returns_df)
        if self.state_vector_df is None: return
        
        self.ksi_series = self._compute_adaptive_ksi()
        if self.ksi_series is None: return
        
        self._log("\n✅ Pipeline completed successfully! KSI Series is ready.", 'SUCCESS')
        
        # --- Run Analysis & Visualization ---
        self._classify_components() # Critical setup step for all analysis
        self.analyze_and_display_peaks()
        self.plot_ksi()

        print("\n" + "="*80)
        print("--- GENERATING VISUAL INTELLIGENCE REPORTS ---")
        print("="*80)
        
        if self.config['visual_intelligence']['create_regime_fingerprints']:
            self._log("Creating Regime Fingerprints dashboard...")
            self.create_regime_fingerprints()
        
        if self.config['visual_intelligence']['create_event_gallery']:
            self._log("Creating Top Stress Event gallery...")
            self.create_event_gallery()
        
        self._log("Creating detailed visual reports for top 5 peaks...")
        top_5_events = self.ksi_series.nlargest(5)
        for i, date in enumerate(top_5_events.index):
            self._log(f"  - Generating report for peak #{i+1} on {date.date()}...")
            self.analyze_single_peak_visual(date)
    
    # --- [Core Pipeline Stages] ---

    def _acquire_data(self):
        self._log("\n--- [1] ACQUIRING DATA ---")
        if self.config['execution']['test_mode']: return self._generate_synthetic_data()
        try:
            self.bql_svc = bql.Service()
            ticker_list = [v['primary'] for v in self.config['assets'].values()]
            req = bql.Request(ticker_list, {'price': self.bql_svc.data.px_last(dates=self.bql_svc.func.range(self.config['date_range']['start'], self.config['date_range']['end']), fill='prev')})
            res = self.bql_svc.execute(req)
            price_df_raw = res[0].df().reset_index().pivot(index='DATE', columns='ID', values='price')
            ticker_to_name = {v['primary']: k for k, v in self.config['assets'].items()}
            price_df = price_df_raw.rename(columns=ticker_to_name)
            self._log(f"✓ Successfully fetched data. Shape: {price_df.shape}")
            return price_df
        except Exception as e:
            self._log_error("BQL_FETCH", "Failed to execute BQL request", e)
            return None

    def _process_data(self, price_df):
        self._log("\n--- [2] PROCESSING DATA ---")
        price_df = price_df.ffill().dropna(how='all')
        if len(price_df) < self.config['model_params']['correlation_window'] + self.config['model_params']['mahalanobis_lookback']:
            self._log_error("DATA_CLEAN", f"Insufficient data points: {len(price_df)}.")
            return None
        returns_df = pd.DataFrame(index=price_df.index)
        for asset in self.config['assets']:
            # For vol indices, simple differences (or log-returns) can be used. Log is often more stable.
            returns_df[asset] = np.log(price_df[asset] / price_df[asset].shift(1))
        returns_df = returns_df.dropna()
        self._log(f"✓ Log returns calculated. Shape: {returns_df.shape}")
        return returns_df

    def _build_state_vector(self, returns_df):
        self._log("\n--- [3] BUILDING KINETIC STATE VECTOR ---")
        p = self.config['model_params']
        window, corr_method, clip = p['correlation_window'], p['correlation_method'], p['max_correlation_clip']
        pairs = list(itertools.combinations(returns_df.columns, 2))
        self.pair_names = [f"{p1}-{p2}" for p1, p2 in pairs]
        corr_df = pd.concat([returns_df[p1].rolling(window).corr(returns_df[p2], method=corr_method) for p1, p2 in pairs], axis=1)
        pos = corr_df.clip(-clip, clip).apply(np.arctanh)
        vel, acc = pos.diff(1), pos.diff(1).diff(1)
        
        # <<< FIX: Assign columns separately for each DataFrame >>>
        pos.columns = [f"pos_{n}" for n in self.pair_names]
        vel.columns = [f"vel_{n}" for n in self.pair_names]
        acc.columns = [f"acc_{n}" for n in self.pair_names]
        
        eigen_list = [np.linalg.eigh(returns_df.iloc[i-window:i].corr(method=corr_method).values)[0][-1] for i in range(window, len(returns_df))]
        lambda_1 = pd.Series(eigen_list, index=returns_df.index[window-1:], name='lambda_max')
        delta_lambda_1 = lambda_1.diff(1).rename('delta_lambda_max')
        state_vector_df = pd.concat([pos, vel, acc, lambda_1, delta_lambda_1], axis=1).dropna()
        self._log(f"✓ State vector created with shape: {state_vector_df.shape}")
        return state_vector_df

    def _compute_adaptive_ksi(self):
        self._log("\n--- [4] COMPUTING ADAPTIVE KSI ---")
        lookback = self.config['model_params']['mahalanobis_lookback']
        S = self.state_vector_df.values
        ksi_values = []
        for t in range(lookback, S.shape[0]):
            try:
                history = S[t - lookback : t]
                mu_hist = np.mean(history, axis=0)
                sigma_hist = GIS(history)
                sigma_inv = np.linalg.pinv(sigma_hist)
                deviation = S[t] - mu_hist
                ksi_squared = deviation.T @ sigma_inv @ deviation
                ksi_values.append(np.sqrt(max(0, ksi_squared)))
            except Exception:
                ksi_values.append(np.nan)
        ksi_series = pd.Series(ksi_values, index=self.state_vector_df.index[lookback:]).interpolate()
        self._log(f"✓ KSI calculation finished. Series length: {len(ksi_series)}")
        return ksi_series

    # --- [Analysis & Interpretation] ---
    
    def analyze_and_display_peaks(self):
        # This method provides the text-based console output for quick review
        print("\n" + "="*80)
        print("--- KINETIC PHASE TRANSITION ANALYSIS (CONSOLE REPORT) ---")
        print("="*80)
        top_events = self.ksi_series.nlargest(self.config['analysis_params']['num_peaks_to_analyze'] * 5)
        peak_dates = []
        for date, _ in top_events.items():
            if len(peak_dates) >= self.config['analysis_params']['num_peaks_to_analyze']: break
            if not any(abs((date - ad).days) < self.config['analysis_params']['peak_separation_days'] for ad in peak_dates):
                peak_dates.append(date)
        for peak_date in sorted(peak_dates):
            self._analyze_single_peak_text(peak_date)

    def _analyze_single_peak_text(self, peak_date):
        # Generates and prints a detailed, interpretable report for a single peak.
        scores_pct = self._get_scores_pct(peak_date)
        verdict, color_code = self._get_verdict(scores_pct)
        ksi_val = self.ksi_series.loc[peak_date]
        print(f"\n--- Peak Event: {peak_date.strftime('%d-%b-%Y')} | KSI: {ksi_val:.2f} ---")
        print(f"Verdict: {color_code} {verdict}")
        print("\n  Diagnostic Scorecard:")
        print(f"    - Cross-Contamination Acceleration: {scores_pct.get('cross_accel', 0):>5.1f}%")
        print(f"    - Other Cross-Pair Kinetics:        {scores_pct.get('cross_other', 0):>5.1f}%")
        print(f"    - Intra-Pair Kinetics:              {scores_pct.get('intra_kinetics', 0):>5.1f}%")
        print(f"    - Global Eigenvalue Dynamics:       {scores_pct.get('global_eigen', 0):>5.1f}%")
        print("-" * 65)

    # --- [Visual Intelligence Suite] ---

    def create_regime_fingerprints(self):
        fig, axes = plt.subplots(2, 2, figsize=(20, 16))
        
        # 1. Correlation Matrix Evolution Heatmap
        ax = axes[0,0]
        pos_cols = [col for col in self.state_vector_df.columns if 'pos_' in col]
        corr_evolution = self.state_vector_df[pos_cols].iloc[::5].apply(np.tanh)
        yticklabels = [f"{self.short_names[p.split('-')[0]]}-{self.short_names[p.split('-')[1]]}" for p in self.pair_names]
        sns.heatmap(corr_evolution.T, ax=ax, cmap='RdBu_r', center=0, vmin=-1, vmax=1, cbar_kws={'label': 'Correlation'}, yticklabels=yticklabels)
        ax.set_title('Correlation Structure Evolution (Policy vs Structural)', fontsize=14)
        
        # 2. Phase Space Plot (Eigenvalue vs Cross-Correlation Acceleration)
        ax = axes[0,1]
        cross_accel_cols = [col for col, cat in self.component_categories.items() if cat == 'cross_accel']
        cross_accel = self.state_vector_df[cross_accel_cols].abs().sum(axis=1)
        scatter = ax.scatter(self.state_vector_df['lambda_max'], cross_accel.reindex(self.state_vector_df.index).fillna(0), c=self.ksi_series, cmap='viridis', alpha=0.6, s=20)
        ax.set_xlabel('Max Eigenvalue (System Integration)', fontsize=12)
        ax.set_ylabel('Cross-Pair Acceleration (Policy↔Structural Contamination)', fontsize=12)
        ax.set_title('Phase Space: When Worlds Collide', fontsize=14)
        plt.colorbar(scatter, ax=ax, label='KSI')
        
        # 3. Kinetic Energy Spectrum
        ax = axes[1,0]
        window = 60
        pos_energy = self.state_vector_df[[c for c in self.state_vector_df.columns if 'pos_' in c]].rolling(window).std().mean(axis=1).fillna(0)
        vel_energy = self.state_vector_df[[c for c in self.state_vector_df.columns if 'vel_' in c]].rolling(window).std().mean(axis=1).fillna(0)
        acc_energy = self.state_vector_df[[c for c in self.state_vector_df.columns if 'acc_' in c]].rolling(window).std().mean(axis=1).fillna(0)
        ax.fill_between(self.state_vector_df.index, 0, pos_energy, alpha=0.4, label='Position Energy (Structure)')
        ax.fill_between(self.state_vector_df.index, pos_energy, pos_energy+vel_energy, alpha=0.4, label='Velocity Energy (Momentum)')
        ax.fill_between(self.state_vector_df.index, pos_energy+vel_energy, pos_energy+vel_energy+acc_energy, alpha=0.4, label='Acceleration Energy (Shock)')
        ax.set_title('Kinetic Energy Decomposition', fontsize=14); ax.legend()
        
        # 4. Event Signature Patterns
        ax = axes[1,1]
        top_events = self.ksi_series.nlargest(10)
        signatures = []
        for date in top_events.index:
            if date in self.state_vector_df.index:
                sig = self.state_vector_df.loc[date].values
                norm = np.linalg.norm(sig)
                signatures.append(sig / norm if norm > 0 else sig)
        if signatures:
            sns.heatmap(np.array(signatures), ax=ax, cmap='coolwarm', center=0, cbar_kws={'label': 'Normalized State'})
            ax.set_title('Top 10 Stress Event Signatures', fontsize=14)
            ax.set_xlabel('State Vector Components'); ax.set_ylabel('Event Rank')
        
        plt.tight_layout(rect=[0, 0.03, 1, 0.97])
        plt.suptitle("Visual Intelligence Dashboard: Regime Fingerprints", fontsize=20)
        plt.savefig('regime_fingerprints.png', dpi=300, bbox_inches='tight')
        plt.show()

    def plot_correlation_network(self, date, ax):
        # Get correlations for this date
        correlations = {}
        for col in self.state_vector_df.columns:
            if 'pos_' in col and date in self.state_vector_df.index:
                pair_full = col.replace('pos_', '')
                correlations[pair_full] = np.tanh(self.state_vector_df.loc[date, col])
        
        G = nx.Graph()
        pos = {'3M2Y': (-1, 1), '1Y5Y': (-1, -1), '5Y10Y': (1, 1), '10Y30Y': (1, -1)}
        
        for pair_full, corr in correlations.items():
            a1_full, a2_full = pair_full.split('-')
            G.add_edge(self.short_names[a1_full], self.short_names[a2_full], weight=corr)
        
        policy_nodes, struct_nodes = ['3M2Y', '1Y5Y'], ['5Y10Y', '10Y30Y']
        nx.draw_networkx_nodes(G, pos, nodelist=policy_nodes, node_color='#FF6B6B', node_size=3000, alpha=0.9, ax=ax)
        nx.draw_networkx_nodes(G, pos, nodelist=struct_nodes, node_color='#4D96FF', node_size=3000, alpha=0.9, ax=ax)
        
        for (u, v, d) in G.edges(data=True):
            width, color = abs(d['weight']) * 8, '#44AF69' if d['weight'] > 0 else '#C34A36'
            alpha = min(abs(d['weight']) + 0.3, 1.0)
            style = 'dashed' if (u in policy_nodes and v in struct_nodes) or (u in struct_nodes and v in policy_nodes) else 'solid'
            nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], width=width, edge_color=color, alpha=alpha, style=style, ax=ax)
        
        nx.draw_networkx_labels(G, pos, font_size=12, font_weight='bold', ax=ax)
        
        ksi_val = self.ksi_series.loc[date] if date in self.ksi_series.index else 0
        ax.text(0.02, 0.98, f'KSI: {ksi_val:.2f}', transform=ax.transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))
        ax.axis('off')

    def create_event_gallery(self):
        top_events = self.ksi_series.nlargest(6)
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.ravel()
        for i, (date, ksi) in enumerate(top_events.items()):
            self.plot_correlation_network(date, ax=axes[i])
            axes[i].set_title(f'{date.strftime("%d-%b-%Y")} (KSI: {ksi:.1f})', fontsize=14)
        plt.suptitle('Gallery of Top Stress Events: Correlation Network Evolution', fontsize=20)
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.savefig('event_gallery.png', dpi=300, bbox_inches='tight')
        plt.show()

    def analyze_single_peak_visual(self, peak_date):
        fig, axes = plt.subplots(2, 2, figsize=(20, 15))
        
        # 1. Component Contribution Pie Chart
        scores_pct = self._get_scores_pct(peak_date)
        colors = {'cross_accel': '#E74C3C', 'cross_other': '#F39C12', 'intra_kinetics': '#F1C40F', 'global_eigen': '#3498DB'}
        ax=axes[0,0]
        ax.pie([scores_pct.get(k,0) for k in colors.keys()], labels=list(colors.keys()), colors=list(colors.values()), autopct='%1.1f%%', startangle=90)
        ax.set_title(f'Stress Decomposition')
        
        # 2. State Vector Heatmap (Z-Score)
        ax=axes[0,1]
        history = self.state_vector_df.loc[:peak_date].iloc[-self.config['model_params']['mahalanobis_lookback']:]
        event_vector = self.state_vector_df.loc[peak_date]
        mu_hist, std_hist = history.mean(), history.std()
        state_norm = ((event_vector - mu_hist) / (std_hist + 1e-9)).values.reshape(1, -1)
        sns.heatmap(state_norm, ax=ax, cmap='RdBu_r', center=0, xticklabels=self.state_vector_df.columns, yticklabels=['Z-Score'])
        ax.set_title('Normalized State Vector (How unusual is each component?)'); plt.setp(ax.get_xticklabels(), rotation=90)
        
        # 3. Time Series Context
        ax = axes[1,0]
        window_days = pd.Timedelta(days=60)
        context_series = self.ksi_series.loc[peak_date-window_days : peak_date+window_days]
        ax.plot(context_series.index, context_series.values, 'k-', linewidth=1.5)
        ax.axvline(peak_date, color='red', linestyle='--', linewidth=2)
        q95 = self.ksi_series.quantile(0.95)
        ax.fill_between(context_series.index, q95, context_series.max(), where=context_series > q95, alpha=0.3, color='red', label=f'Above 95th Pctile ({q95:.2f})')
        ax.set_title('KSI Time Series Context'); ax.legend(); ax.set_yscale('log')
        
        # 4. Correlation Network for this date
        self.plot_correlation_network(peak_date, ax=axes[1,1])
        axes[1,1].set_title("Correlation Network on Event Date")
        
        plt.suptitle(f'Visual Analysis: {peak_date.strftime("%d-%b-%Y")} Stress Event', fontsize=20)
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.show()

    def _get_scores_pct(self, peak_date):
        """Refactored helper to calculate diagnostic scores for any peak date."""
        try:
            peak_idx = self.state_vector_df.index.get_loc(peak_date)
            lookback = self.config['model_params']['mahalanobis_lookback']
            history = self.state_vector_df.iloc[peak_idx - lookback : peak_idx].values
            event_vector = self.state_vector_df.iloc[peak_idx].values
            mu_hist = np.mean(history, axis=0)
            sigma_hist = GIS(history)
            eigenvalues, eigenvectors = np.linalg.eigh(sigma_hist)
            y = eigenvectors.T @ (event_vector - mu_hist)
            mode_contributions = (y**2) / np.maximum(eigenvalues, 1e-12)
            scores = {'cross_accel': 0, 'cross_other': 0, 'intra_kinetics': 0, 'global_eigen': 0}
            for mode_idx, contribution in enumerate(mode_contributions):
                eigenvector = eigenvectors[:, mode_idx]
                for i, comp_name in enumerate(self.state_vector_df.columns):
                    loading_sq = eigenvector[i]**2
                    cat = self.component_categories.get(comp_name, 'other')
                    scores[cat] += contribution * loading_sq
            total_score = sum(scores.values())
            return {k: (v / total_score) * 100 if total_score > 0 else 0 for k, v in scores.items()}
        except Exception as e:
            self._log_error("SCORE_CALC", f"Could not calculate scores for {peak_date.date()}", e)
            return {}

    def _classify_components(self):
        self.component_categories = {}
        assets_cfg = self.config['assets']
        for col in self.state_vector_df.columns:
            if 'lambda' in col:
                self.component_categories[col] = 'global_eigen'
                continue
            parts = col.split('_')
            kinetic_type, pair_name = parts[0], '-'.join(parts[1].split('-'))
            if '-' in pair_name:
                p1, p2 = pair_name.split('-')
                if p1 in assets_cfg and p2 in assets_cfg:
                    if assets_cfg[p1]['group'] == assets_cfg[p2]['group']:
                        self.component_categories[col] = 'intra_kinetics'
                    else:
                        self.component_categories[col] = 'cross_accel' if kinetic_type == 'acc' else 'cross_other'

    def _get_verdict(self, scores_pct):
        threshold = self.config['analysis_params']['contamination_threshold_pct']
        if scores_pct.get('cross_accel', 0) >= threshold:
            return "KINETIC CROSS-CONTAMINATION EVENT (High Confidence)", "🔴"
        elif scores_pct.get('cross_accel', 0) + scores_pct.get('cross_other', 0) >= threshold:
            return "STRUCTURAL TENSION WARNING (Moderate Confidence)", "🟠"
        elif scores_pct.get('intra_kinetics', 0) >= threshold:
            return "CONTAINED REGIME STRESS (Low Systemic Risk)", "🟡"
        else:
            return "UNDEFINED STRESS (No Clear Pattern)", "⚪️"

    def plot_ksi(self):
        if self.ksi_series.empty: return
        self._log("\n--- [5] GENERATING PLOT ---")
        plt.style.use('seaborn-v0_8-whitegrid')
        fig, ax = plt.subplots(figsize=(16, 8))
        self.ksi_series.plot(ax=ax, color='k', linewidth=1.5, label='KSI (v13.0 - IR Vol)')
        q95 = self.ksi_series.quantile(0.95)
        ax.axhline(q95, color='darkorange', linestyle='--', linewidth=1.2, label=f'95th Percentile ({q95:.2f})')
        ax.set_title('Kinetic Stress Index (Interest Rate Volatility)', fontsize=18)
        ax.set_ylabel('KSI (Unitless Stress Level, Log Scale)', fontsize=12)
        ax.set_yscale('log'); ax.legend(loc='upper left')
        plt.tight_layout(); plt.show()

    # --- [Helper & Utility Methods] ---
    
    def _log(self, message, level='INFO'):
        if self.config['execution']['verbose'] or level != 'INFO':
            print(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")

    def _log_error(self, context, message, exception=None):
        error_msg = f"❌ ERROR [{context}]: {message}" + (f" | {type(exception).__name__}: {str(exception)}" if exception else "")
        self._log(error_msg, 'ERROR')

    def _generate_synthetic_data(self):
        self._log("Generating synthetic test data...")
        dates = pd.date_range(start=self.config['date_range']['start'], end=self.config['date_range']['end'], freq='B')
        n_days = len(dates)
        np.random.seed(42)
        returns = pd.DataFrame(np.random.randn(n_days, 4) * 0.02, columns=self.config['assets'].keys())
        price_df = 10 * np.exp(returns.cumsum())
        self._log(f"✓ Generated {len(price_df)} days of synthetic data.")
        return price_df

    def _validate_environment(self):
        if not BQL_AVAILABLE and not self.config['execution']['test_mode']:
            self._log_error("ENV", "BQL not available and test_mode is False. Cannot proceed.")
            return False
        return True

if __name__ == '__main__':
    try:
        analyzer = KineticPhaseTransitionAnalyzer(CONFIG)
        analyzer.run()
    except Exception as e:
        print(f"\n--- A CRITICAL ERROR OCCURRED DURING EXECUTION ---")
        traceback.print_exc()

